{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model performance\n",
    "\n",
    "In this notebook, we are going to load in some new data and test how well our ensemble of random forest models predicts if users will own certain games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pickle\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import ast\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define SQL database info\n",
    "db_name  = 'UserInfo'\n",
    "username = 'username'\n",
    "host     = 'localhost'\n",
    "pwd      = 'password'\n",
    "port     = '5432'\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(username, pwd, host, port, db_name))\n",
    "print(engine.url)\n",
    "\n",
    "# connect to database:\n",
    "con = None\n",
    "con = psycopg2.connect(database = db_name, user = username, password = pwd, host = host)\n",
    "cur = con.cursor() # get a cursor to our current connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numUsers = 1000 # Number of users to train the model\n",
    "# make a query to initialize the allPlayers table\n",
    "create_table_sql = \"\"\"\n",
    "SELECT * FROM allPlayers WHERE gameCount IS NOT NULL ORDER BY random() LIMIT {};\n",
    "\"\"\".format(numUsers)\n",
    "userFrame = pd.read_sql_query(create_table_sql,con)\n",
    "\n",
    "# For now we have to remove users that have the same game information (earlier error in writing data to database)\n",
    "userFrame = userFrame.drop_duplicates(subset='games',keep=\"last\")\n",
    "len(userFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in TF-IDF corpus and dictionary\n",
    "\n",
    "This will be used to compute a similarity score between games to be tested, and the games that each user currently owns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the pretrained tf-idf models,dictionary, and corpus for game tags and game descriptions\n",
    "tag_dictionary = corpora.Dictionary.load('/home/gameTagDict.dict')\n",
    "tag_corpus     = corpora.MmCorpus('/home/gameTagCorpus.mm')\n",
    "description_dictionary = corpora.Dictionary.load('/home/gameDescriptionDict.dict')\n",
    "description_corpus     = corpora.MmCorpus('/home/gameDescriptionCorpus.mm')\n",
    "\n",
    "# Initialise a term-frequency inverse document frequency model based on the corpus for both tags and descriptions\n",
    "tag_tfidf = models.TfidfModel(tag_corpus) \n",
    "description_tfidf =  models.TfidfModel(description_corpus)\n",
    "# Initialise a similarity index using the stored index, corpus, and dictionary\n",
    "tag_sims = similarities.Similarity('/home/',\n",
    "                                   tag_tfidf[tag_corpus],num_features=len(tag_dictionary))\n",
    "description_sims = similarities.Similarity('/home/',\n",
    "                    description_tfidf[description_corpus],num_features=len(description_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's load in the game data\n",
    "# make a query to initialize the allGames table\n",
    "create_table_sql = \"\"\"\n",
    "SELECT * FROM allGames WHERE gamename IS NOT NULL;\n",
    "\"\"\"\n",
    "gameFrame = pd.read_sql_query(create_table_sql,con)\n",
    "gameFrame = gameFrame.dropna(axis=0, how='any') # Remove any rows that have nans\n",
    "len(gameFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to compute similarity of game descriptions and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allGames = gameFrame.gameid.tolist()\n",
    "def compute_tag_similarity(testGame,userGameDict,ind):\n",
    "    # This function comutes the similarity of one game to the games a user already owns based on game tags\n",
    "    testRow     = gameFrame[gameFrame['gameid']==testGame]\n",
    "    tmp         = testRow.tags.tolist()\n",
    "    currentTags = ast.literal_eval(tmp[0]) # Evaluate to get in list form\n",
    "    query       = ' '.join(currentTags) # Join all together with spaces\n",
    "    print(\"Computing tag similarity for user {} and game {}\".format(ind,testGame),end='\\r')\n",
    "    \n",
    "    # Tokenize\n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "    # Compare to dictionary we computed on all other games\n",
    "    query_doc_bow = tag_dictionary.doc2bow(query_doc)\n",
    "    # Vectorize word representation\n",
    "    query_doc_tf_idf = tag_tfidf[query_doc_bow]\n",
    "    # Compute the similarity of the game with current tags to al games in our database\n",
    "    query_similarity = tag_sims[query_doc_tf_idf]\n",
    "\n",
    "    # Control for if the user has only one game\n",
    "    if type(userGameDict) == dict:\n",
    "        userGameDict = list([userGameDict])\n",
    "        \n",
    "    gamesOwned  = []\n",
    "    gamesPlayed = []\n",
    "    for g in userGameDict:\n",
    "        if g['appid'] in allGames:\n",
    "            if g['appid'] != testGame: # If the user owns the testGame, we should omit this from the model\n",
    "                gamesOwned.append(g['appid'])\n",
    "                gamesPlayed.append(g['playtime_forever']+0.01) # Add small amount to avoid dividing by zero\n",
    "    # normalize the playtime to the total amount of playtime\n",
    "    gamesPlayed = np.array([x / sum(gamesPlayed) for x in gamesPlayed]) \n",
    "    \n",
    "    # If the user owns the testGame, we should omit this from the model\n",
    "    if testGame in gamesOwned:\n",
    "        gamesOwned.remove(testGame)\n",
    "    \n",
    "    # We only want to compare to games that are in our original list\n",
    "    testInd = []\n",
    "    for game in gamesOwned:\n",
    "        if game in allGames:\n",
    "            testInd.append(allGames.index(game))\n",
    "    user2game_similarity = query_similarity[testInd]\n",
    "    user2game_weighted_similarity = user2game_similarity*gamesPlayed\n",
    "    \n",
    "    # If we just have empty arrays, then fill with 0\n",
    "    if user2game_similarity.size < 1:\n",
    "        user2game_similarity = 0\n",
    "        user2game_weighted_similarity = 0\n",
    "        \n",
    "    return np.median(user2game_similarity)#, np.median(user2game_weighted_similarity)\n",
    "\n",
    "def preprocess_descriptions(description):\n",
    "    # Define a list of words that we will not include, since they do not convey meaning\n",
    "    try:\n",
    "        removelist = set('for a of the and to in on is or be as where it its at an - with by'.split())\n",
    "        description = re.sub(r\"\\s+\", \" \", description) # remove line breaks and tabs\n",
    "        description = re.sub(r\"<.*?>\", \"\", description) # remove <>\n",
    "        description = description.replace(\",\",\"\") # remove commas\n",
    "        description = description.replace(\".\",\"\") # remove periods\n",
    "        # Split text into a list of words\n",
    "        text = [word for word in description.lower().split() if word not in removelist]\n",
    "        return text\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def compute_description_similarity(testGame,userGameDict,ind):\n",
    "    # This function comutes the similarity of one game to the games a user already owns based on game description\n",
    "    testRow     = gameFrame[gameFrame['gameid']==testGame]\n",
    "    tmp         = testRow.detailedescription.tolist()\n",
    "    query_doc   = preprocess_descriptions(tmp[0]) # preprocess data\n",
    "    print(\"Computing description similarity for user {} and game {}\".format(ind,testGame),end='\\r')\n",
    "    # Compare to dictionary we computed on all other games\n",
    "    query_doc_bow = description_dictionary.doc2bow(query_doc)\n",
    "    # Vectorize word representation\n",
    "    query_doc_tf_idf = description_tfidf[query_doc_bow]\n",
    "    # Compute the similarity of the game with current tags to al games in our database\n",
    "    query_similarity = description_sims[query_doc_tf_idf]\n",
    "    \n",
    "    # Control for if the user has only one game\n",
    "    if type(userGameDict) == dict:\n",
    "        userGameDict = list([userGameDict])\n",
    "    \n",
    "    gamesOwned  = []\n",
    "    gamesPlayed = []\n",
    "    for g in userGameDict:\n",
    "        if g['appid'] in allGames:\n",
    "            if g['appid'] != testGame: # If the user owns the testGame, we should omit this from the model\n",
    "                gamesOwned.append(g['appid'])\n",
    "                gamesPlayed.append(g['playtime_forever']+0.01)\n",
    "    # normalize the playtime to the total amount of playtime\n",
    "    gamesPlayed = np.array([x / sum(gamesPlayed) for x in gamesPlayed]) \n",
    "    \n",
    "    # We only want to compare to games that are in our original list\n",
    "    testInd = []\n",
    "    for game in gamesOwned:\n",
    "        if game in allGames:\n",
    "            testInd.append(allGames.index(game))\n",
    "    user2game_similarity = query_similarity[testInd]\n",
    "    user2game_weighted_similarity = user2game_similarity*gamesPlayed\n",
    "    \n",
    "    # If we just have empty arrays, then fill with 0\n",
    "    if user2game_similarity.size < 1:\n",
    "        user2game_similarity = 0\n",
    "        user2game_weighted_similarity = 0\n",
    "    \n",
    "    return np.median(user2game_similarity)#, np.median(user2game_weighted_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testFrame = pd.DataFrame()\n",
    "testFrame['timecreated'] = userFrame.timecreated\n",
    "testFrame['usercountry'] = userFrame.loccountrycode\n",
    "testFrame['games']       = userFrame.games\n",
    "\n",
    "# Initialise lists to fill dataframe\n",
    "numFriends   = []\n",
    "numGames     = []\n",
    "playTimeList = []\n",
    "gameIDs      = []\n",
    "for index, row in userFrame.iterrows():\n",
    "    playTime   = 0 # Default\n",
    "    gamesOwned = []\n",
    "    try:\n",
    "        friendDict = ast.literal_eval(row['friends'])\n",
    "        numFriends.append(len(friendDict))\n",
    "    except:\n",
    "        numFriends.append(0) # If code throws an error, it is because user has no games\n",
    "    try:\n",
    "        gameDict = ast.literal_eval(row['games'])\n",
    "        numGames.append(len(gameDict))\n",
    "        for g in gameDict:\n",
    "            playTime += g['playtime_forever']/60 # divide by 60 to get time expressed in hours\n",
    "            gamesOwned.append(g['appid'])\n",
    "        gameIDs.append(gamesOwned)\n",
    "    except:\n",
    "        numGames.append(0) # If code throws an error, it is because the user has no games\n",
    "        gameIDs.append(0)\n",
    "    playTimeList.append(playTime)    \n",
    "        \n",
    "\n",
    "# Now we fill in the columns of the dataframe with the lists we just compted\n",
    "testFrame['friends']    = numFriends\n",
    "testFrame['playTime']   = playTimeList\n",
    "testFrame['numGames']   = numGames\n",
    "testFrame['gamesOwned'] = gameIDs\n",
    "testFrame = testFrame.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring user and game data together in one frame\n",
    "\n",
    " Things to achieve in the cell below:\n",
    " - fill in a pandas dataframe with user information that we already have\n",
    " - into the same dataframe insert game inforamation\n",
    " - compute similarity index for tags and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testData = testFrame[:numUsers]\n",
    "test     = pd.DataFrame() # initialise dataframe\n",
    "\n",
    "for index,user in testData.iterrows():    \n",
    "    # Define the users game dict that contains games owned info and hours played\n",
    "    gd = ast.literal_eval(user.games)\n",
    "\n",
    "    # This code detects which games the user owns, and generates a list of games they don't own\n",
    "    # To deal with imbalance in classes, we generate a list of games that uers don't own that is \n",
    "    # the same length of the list of games that they do own. \n",
    "    gmOwn = []\n",
    "    for g in gd:\n",
    "        if g['appid'] in allGames:\n",
    "            gmOwn.append(g['appid'])\n",
    "    gamesNotOwned = [x for x in allGames if x not in gmOwn]\n",
    "    #gamesNotOwned = random.choices(gamesNotOwned,k=len(gmOwn))\n",
    "    \n",
    "    if gmOwn: # We only want to process the user that have at least one game\n",
    "\n",
    "        keepColumns = ['timecreated','usercountry','friends','playTime','numGames']\n",
    "        userGames = gameFrame.loc[gameFrame['gameid'].isin(gmOwn)]\n",
    "        userGames.loc[:,'userHasGame'] = pd.Series(np.ones(len(userGames)), index=userGames.index)\n",
    "        testGames = gameFrame.loc[gameFrame['gameid'].isin(gamesNotOwned)]\n",
    "        testGames.loc[:,'userHasGame'] = pd.Series(np.zeros(len(testGames)),index=testGames.index)\n",
    "        tmp = pd.concat([userGames,testGames],axis=0)\n",
    "        frame = pd.concat([pd.DataFrame(testData.loc[index,keepColumns]).T]*len(allGames))\n",
    "\n",
    "        # Reindex both tmp and frame\n",
    "        tmp   = tmp.reset_index()\n",
    "        frame = frame.reset_index()\n",
    "        userPlusGame = pd.concat([frame,tmp],axis=1)\n",
    "        \n",
    "       # For some strange reason I was still getting some NaN's. let's just make sure they are not included\n",
    "        userPlusGame = userPlusGame.dropna(axis=0, how='any') # Remove any rows that have nans\n",
    "        \n",
    "        # Now we compute the td-idf similarity scores \n",
    "        userPlusGame['description_similarity'] = userPlusGame.apply(lambda row: compute_description_similarity(row['gameid'], gd, index), axis=1)\n",
    "        userPlusGame['tag_similarity'] = userPlusGame.apply(lambda row: compute_tag_similarity(row['gameid'], gd, index), axis=1)\n",
    "         \n",
    "        # Now I can append this to the end of the dataframe\n",
    "        test = test.append(userPlusGame)\n",
    "\n",
    "        print(index,end='\\r')\n",
    "    # We will test for each game in our game list, so we need to duplicate the user information n*games number of times\n",
    "    #test = test.append([testData.loc[[index]]]*len(userGames)*2)\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataframe to be suitable for modeling\n",
    "\n",
    "Our model can only process numbers, so we have to convert categorical variables (country, genre, etc) into so-called 'dummy variables' which essentially converts a list of categorical variables into a much large sparse matrix of 0's and 1's (where each column now represents each instance of a categorical variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_columns(df):\n",
    "    # We also have to remove any weird characters from the dataframe column names\n",
    "    df.columns = [col.replace(',', '') for col in df.columns]\n",
    "    df.columns = [col.replace('[', '') for col in df.columns]\n",
    "    df.columns = [col.replace(']', '') for col in df.columns]\n",
    "    df.columns = [col.replace('<', '') for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def process_frame(df):\n",
    "    \n",
    "    # Generate dummy dataframes from our categorical variables    \n",
    "    dfCountry   = pd.get_dummies(df['usercountry'])\n",
    "    dfCountry   = preprocess_columns(dfCountry)\n",
    "    dfDeveloper = pd.get_dummies(df['developers'])\n",
    "    dfDeveloper = preprocess_columns(dfDeveloper)\n",
    "    dfGenre     = pd.get_dummies(df['genre'])\n",
    "    dfGenre     = preprocess_columns(dfGenre)\n",
    "\n",
    "    # Concatinate the dummy variables with the original dataframe\n",
    "    df_new      = pd.concat([df, dfCountry, dfDeveloper, dfGenre], axis=1)\n",
    "    # Remove the original categorical columns from the dataframe, keeping only the dummy variable columns\n",
    "    df_new = df_new.drop(['usercountry','developers','genre','tags'],axis=1)\n",
    "\n",
    "    # We have to convert the data type to float so that the XGBoost model can handle it\n",
    "    df_new['timecreated'] = pd.to_numeric(df_new['timecreated'],downcast='float')\n",
    "    df_new['friends'] = pd.to_numeric(df_new['friends'],downcast='float')\n",
    "    df_new['playTime'] = pd.to_numeric(df_new['playTime'],downcast='float')\n",
    "    df_new['numGames'] = pd.to_numeric(df_new['numGames'],downcast='float')\n",
    "    return df_new\n",
    "\n",
    "def add_missing_dummy_columns( d, columns ):\n",
    "    # This script detects missing columns in the test dataframe, and adds them\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0\n",
    "\n",
    "def fix_columns( d, columns ):  \n",
    "    # This function fixes columns in the the test dataframe to be the same as what the training dataset. \n",
    "    add_missing_dummy_columns( d, columns )\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert( set( columns ) - set( d.columns ) == set())\n",
    "\n",
    "    extra_cols = set( d.columns ) - set( columns )\n",
    "    if extra_cols:\n",
    "        print(\"extra columns: {}\".format(extra_cols))\n",
    "\n",
    "    d = d[ columns ]\n",
    "    return d\n",
    "\n",
    "# Process the dataframes by creating dummy variabels etc.\n",
    "df_new = process_frame(test)\n",
    "\n",
    "# Now we are going to load in some rows from the dataframe that was used to train the model.\n",
    "testSet  = pd.read_csv('/home/trainingDataset',nrows=20)\n",
    "del testSet['Unnamed: 0']\n",
    "testCols = testSet.columns\n",
    "\n",
    "# Using the above functions, we will ensure that the test dataframes are in the shape the model is expecting\n",
    "df_new = fix_columns(df_new,testCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of feature headers\n",
    "featureHeaders = list(df_new.columns)\n",
    "targetFeature = 'userHasGame'\n",
    "featureHeaders.remove(targetFeature)\n",
    "mkList = []\n",
    "for x in featureHeaders:\n",
    "    mkList.append(x)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelList = pickle.load(open('/home/ensemble_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through models and compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred  = np.zeros((len(modelList),len(df_new)))\n",
    "prob  = np.zeros((len(modelList),len(df_new)))\n",
    "for ind,model in enumerate(modelList):\n",
    "    pred[ind,:] = model.predict(df_new[mkList])\n",
    "    tmp = model.predict_proba(df_new[mkList])\n",
    "    prob[ind,:] = tmp[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.round(np.median(pred,axis=0))\n",
    "\n",
    "Accuracy = accuracy_score(df_new[targetFeature], predictions)\n",
    "rc = recall_score(df_new[targetFeature],predictions)\n",
    "pc = precision_score(df_new[targetFeature],predictions)\n",
    "print(\"Accuracy = {}\".format(np.round(Accuracy,2)))\n",
    "print(\"Precision = {}\".format(np.round(pc,2)))\n",
    "print(\"Recall = {}\".format(np.round(rc,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function for plotting the confusion matrix\n",
    "\n",
    "A confsusion matrix is a well known plot that can be used for assessing the performance of a classification model by summarizing how well the model correctly assigns classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap,vmin=0,vmax=1)\n",
    "    plt.title(title,fontsize=15)\n",
    "    plt.colorbar(shrink=0.7)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45,fontsize=12)\n",
    "    plt.yticks(tick_marks, classes,rotation=45,fontsize=12)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize=15,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the confusion matrix of the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_new[targetFeature],predictions)\n",
    "plt.figure(figsize=(5,5),dpi=100)\n",
    "plot_confusion_matrix(cm, classes=['User owns game','User does not'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ROC curve \n",
    "\n",
    "The ROC curve, or receive operating characteristic, is a great measure for quickly assessing the performance of a classification model. This plot shows how the model specificity (ability to detect classes) changes as a function of the threshold for classification. By plotting like this, we can observe the tradeoff between false positive rate and true positive rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = np.mean(prob,axis=0)\n",
    "fpr, tpr, _ = metrics.roc_curve(df_new[targetFeature], probabilities)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 5),dpi=100)\n",
    "plt.plot(fpr, tpr, color='navy',\n",
    "             lw=3, label='ROC curve (area = {})'.format(round(roc_auc,2)))\n",
    "plt.plot([0, 1], [0, 1], color='darkred', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate',fontsize=15)\n",
    "plt.ylabel('True Positive Rate',fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc=\"lower right\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
