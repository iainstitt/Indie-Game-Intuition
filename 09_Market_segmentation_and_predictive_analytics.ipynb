{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Steam market segmentation and predictive analytics\n",
    "\n",
    "In this notebook, we will load in user information and perform an unsupervised segmentation of the Steam market based on user behavior. Maroe specfically, we will use a non-linear dimensionality reduction technique (tSNE) to visualize how Steam users play games that were released after the year 2010. We will then load in our model that was generated in the \"Train model\" notebook, and use this model to predict how two different hypothetical games will engage the Steam marketplace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's an impressive list of imports.\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "# We import sklearn.\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.manifold.t_sne import (_joint_probabilities,\n",
    "                                    _kl_divergence)\n",
    "from sklearn.utils.extmath import _ravel\n",
    "\n",
    "# We'll use matplotlib for graphics.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import ast\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "# We import seaborn to make some matrix plots\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "# We'll generate an animation with matplotlib and moviepy.\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "# import custom module for processing data\n",
    "import steamProcessingFunctions as spf\n",
    "#import steamPlottingFunctions as steamPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database containing game and user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SQL database info\n",
    "db_name  = 'UserInfo'\n",
    "username = 'username'\n",
    "host     = 'localhost'\n",
    "pwd      = 'password'\n",
    "port     = '5432'\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(username, pwd, host, port, db_name))\n",
    "print(engine.url)\n",
    "\n",
    "# connect to database:\n",
    "con = None\n",
    "con = psycopg2.connect(database = db_name, user = username, password = pwd, host = host)\n",
    "cur = con.cursor() # get a cursor to our current connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query users from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numUsers = 15000 # Nuber of users we want to use for tSNE\n",
    "# make a query to initialize the allPlayers table\n",
    "create_table_sql = \"\"\"\n",
    "SELECT * FROM allPlayers WHERE gameCount IS NOT NULL ORDER BY random() LIMIT {};\n",
    "\"\"\".format(numUsers)\n",
    "userFrame = pd.read_sql_query(create_table_sql,con)\n",
    "#userFrame = userFrame.dropna(axis=0, how='any') # Remove any rows that have nans\n",
    "\n",
    "delCols = ['unnamed', 'avatar', 'avatarfull', 'avatarmedium', 'commentpermission',\n",
    "       'communityvisibilitystate', 'gameextrainfo', 'gameid', 'gameserverip',\n",
    "       'gameserversteamid', 'lastlogoff','locstatecode', 'personaname', 'personastate',\n",
    "       'personastateflags', 'primaryclanid', 'profilestate', 'profileurl',\n",
    "       'realname','loccityid','lobbysteamid','playtime_total']\n",
    "userFrame = userFrame.drop(delCols,axis=1)\n",
    "userFrame = userFrame.dropna(axis=0, how='any') # Remove any rows that have nans\n",
    "print(\"processing {} users\".format(len(userFrame)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the information of games that were released after 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's load in the game data\n",
    "# make a query to initialize the allGames table\n",
    "dateLim = 20100000 # let's only consider games that were released after 2010\n",
    "create_table_sql = \"\"\"\n",
    "SELECT * FROM allGames WHERE gamename IS NOT NULL AND releasedate > {};\n",
    "\"\"\".format(dateLim)\n",
    "gameFrame = pd.read_sql_query(create_table_sql,con)\n",
    "gameFrame = gameFrame.dropna(axis=0, how='any') # Remove any rows that have nans\n",
    "games = list(gameFrame['gameid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring together user and game information\n",
    "\n",
    "In the cell below we build two matrices:\n",
    "- userHasGame is a binary matrix indicating whether each user (row) owns a particular game (column)\n",
    "- userPlaysGame is similar to userHasGame matrix but contains the amount of time (in hours) each user has played each game\n",
    "\n",
    "In addition to these matrices, we also build a dataframe containing the user information, that we will process further in subsequent cells to build it into a shape that the model is expecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise two matrices with dimensions (users , games)\n",
    "# We are going to fill one matrix with a binary condition: if the users has each game or not (1 or 0)\n",
    "# We are going to fill the second matrix with the amount of time (in hours) users play each game\n",
    "userHasGame    = np.zeros((len(userFrame),len(games)))\n",
    "userPlaysGame  = np.zeros((len(userFrame),len(games)))\n",
    "\n",
    "# We are also going to be recording every users first and second most played games, as weel as the amount of time spent playing CS:GO\n",
    "mostPlayedGame = []\n",
    "secondMostPlayedGame = []\n",
    "csgoTime             = []\n",
    "\n",
    "# Initialise a dataframe with basic user information\n",
    "testFrame = pd.DataFrame()\n",
    "testFrame['timecreated'] = userFrame.timecreated\n",
    "testFrame['usercountry'] = userFrame.loccountrycode\n",
    "testFrame['games']       = userFrame.games\n",
    "\n",
    "# Initialise lists to fill dataframe\n",
    "numFriends   = []\n",
    "numGames     = []\n",
    "playTimeList = []\n",
    "gameIDs      = []\n",
    "\n",
    "userCount = -1 # counter\n",
    "for index,row in userFrame.iterrows():\n",
    "    userCount = userCount+1\n",
    "    print(\"Processing user {}\".format(userCount),end='\\r')\n",
    "    try: # To account for errors if users don't have any games\n",
    "        userGames = ast.literal_eval(row['games']) # This is a dict with infor 'appid' and 'playtime_forever' for each game the user ows\n",
    "        usrAppID  = []\n",
    "        usrAppT   = []\n",
    "        for gm in userGames: # Loop through each game the user owns\n",
    "            usrAppID.append(gm['appid']) # gane ID\n",
    "            usrAppT.append(gm['playtime_forever']) # Time spent playing the game\n",
    "        gameCount = -1 # game counter\n",
    "        for cnt,game in enumerate(games): # Loop through each of the games in our game list\n",
    "            gameCount = gameCount+1\n",
    "            if game in usrAppID: # If the user owns that particular game\n",
    "                ind = usrAppID.index(game) # Index of the game in the user game list\n",
    "                userHasGame[userCount,gameCount] = 1 # Fill the relevant entry in the userHasGame matrix with 1\n",
    "                userPlaysGame[userCount,gameCount] = usrAppT[ind] # Fill the relevant entry in the userPlaysGame matrix with the time they have spent playing that game\n",
    "        # Reorder the game lists based on how much users play\n",
    "        sortedGames = [x for _,x in sorted(zip(usrAppT,usrAppID))]\n",
    "        # Note each users most played game, and second most played game\n",
    "        mostPlayedGame.append(sortedGames[-1])\n",
    "        if len(sortedGames) > 1:\n",
    "            secondMostPlayedGame.append(sortedGames[-2])\n",
    "        else:\n",
    "            secondMostPlayedGame.append(0)\n",
    "        #mostPlayedGame.append(usrAppID[usrAppT.index(max(usrAppT))]) # Store the most played game for each user\n",
    "        \n",
    "        # Now detect the time spent playing CS:GO\n",
    "        if 730 in usrAppID:\n",
    "            ind = usrAppID.index(730)\n",
    "            csgoTime.append(np.log10(usrAppT[ind]/60))\n",
    "        else:\n",
    "            csgoTime.append(0)\n",
    "        \n",
    "        # This part grabs the useser info\n",
    "        playTime   = 0 # Default\n",
    "        gamesOwned = []\n",
    "        try:\n",
    "            friendDict = ast.literal_eval(row['friends'])\n",
    "            numFriends.append(len(friendDict))\n",
    "        except:\n",
    "            numFriends.append(0) # If code throws an error, it is because user has no games\n",
    "        try:\n",
    "            gameDict = ast.literal_eval(row['games'])\n",
    "            numGames.append(len(gameDict))\n",
    "            for g in gameDict:\n",
    "                playTime += g['playtime_forever']/60 # divide by 60 to get time expressed in hours\n",
    "                gamesOwned.append(g['appid'])\n",
    "            gameIDs.append(gamesOwned)\n",
    "        except:\n",
    "            numGames.append(0) # If code throws an error, it is because the user has no games\n",
    "            gameIDs.append(0)\n",
    "        playTimeList.append(playTime)    \n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# Now we fill in the columns of the dataframe with the lists we just compted\n",
    "testFrame['friends']    = numFriends\n",
    "testFrame['playTime']   = playTimeList\n",
    "testFrame['numGames']   = numGames\n",
    "testFrame['gamesOwned'] = gameIDs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the tf-idf models for game tag and descriptions\n",
    "\n",
    "These models were generated in a separate script called \"Train TF-IDF model (tags)\" and \"Train TF-IDF model (description)\". These models will be used to engineer the similarity of the hypothetical games to the games that each user currently owns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the pretrained tf-idf models,dictionary, and corpus for game tags and game descriptions\n",
    "tag_dictionary = corpora.Dictionary.load('/home/gameTagDict.dict')\n",
    "tag_corpus     = corpora.MmCorpus('/home/gameTagCorpus.mm')\n",
    "description_dictionary = corpora.Dictionary.load('/home/gameDescriptionDict.dict')\n",
    "description_corpus     = corpora.MmCorpus('/home/gameDescriptionCorpus.mm')\n",
    "\n",
    "# Initialise a term-frequency inverse document frequency model based on the corpus for both tags and descriptions\n",
    "tag_tfidf = models.TfidfModel(tag_corpus) \n",
    "description_tfidf =  models.TfidfModel(description_corpus)\n",
    "# Initialise a similarity index using the stored index, corpus, and dictionary\n",
    "tag_sims = similarities.Similarity('/home/',\n",
    "                                   tag_tfidf[tag_corpus],num_features=len(tag_dictionary))\n",
    "description_sims = similarities.Similarity('/home/',\n",
    "                    description_tfidf[description_corpus],num_features=len(description_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Half-Life 3 and Euro Trucker 3 game hypothetical game features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hl3frame = pd.read_csv('/home/hl3frame')\n",
    "del hl3frame['Unnamed: 0']\n",
    "euroTruck3 = pd.read_csv('/home/euroTruck3')\n",
    "del euroTruck3['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to compute tag and description similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allGames = gameFrame.gameid.tolist()\n",
    "def compute_tag_similarity(testRow,userGameDict):\n",
    "    # This function comutes the similarity of one game to the games a user already owns based on game tags\n",
    "    tmp         = testRow.tags.tolist()\n",
    "    currentTags = ast.literal_eval(tmp[0]) # Evaluate to get in list form\n",
    "    query       = ' '.join(currentTags) # Join all together with spaces\n",
    "\n",
    "    # Tokenize\n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "    # Compare to dictionary we computed on all other games\n",
    "    query_doc_bow = tag_dictionary.doc2bow(query_doc)\n",
    "    # Vectorize word representation\n",
    "    query_doc_tf_idf = tag_tfidf[query_doc_bow]\n",
    "    # Compute the similarity of the game with current tags to al games in our database\n",
    "    query_similarity = tag_sims[query_doc_tf_idf]\n",
    "\n",
    "    # Control for if the user has only one game\n",
    "    if type(userGameDict) == dict:\n",
    "        userGameDict = list([userGameDict])\n",
    "        \n",
    "    gamesOwned  = []\n",
    "    gamesPlayed = []\n",
    "    for g in userGameDict:\n",
    "        if g['appid'] in allGames:\n",
    "            #if g['appid'] != testGame: # If the user owns the testGame, we should omit this from the model\n",
    "            gamesOwned.append(g['appid'])\n",
    "            gamesPlayed.append(g['playtime_forever']+0.01) # Add small amount to avoid dividing by zero\n",
    "    # normalize the playtime to the total amount of playtime\n",
    "    gamesPlayed = np.array([x / sum(gamesPlayed) for x in gamesPlayed]) \n",
    "    \n",
    "    \n",
    "    # We only want to compare to games that are in our original list\n",
    "    testInd = []\n",
    "    for game in gamesOwned:\n",
    "        if game in allGames:\n",
    "            testInd.append(allGames.index(game))\n",
    "    user2game_similarity = query_similarity[testInd]\n",
    "    user2game_weighted_similarity = user2game_similarity*gamesPlayed\n",
    "    \n",
    "    # If we just have empty arrays, then fill with 0\n",
    "    if user2game_similarity.size < 1:\n",
    "        user2game_similarity = 0\n",
    "        user2game_weighted_similarity = 0\n",
    "        \n",
    "    return np.mean(user2game_similarity)#, np.median(user2game_weighted_similarity)\n",
    "\n",
    "def preprocess_descriptions(description):\n",
    "    # Define a list of words that we will not include, since they do not convey meaning\n",
    "    try:\n",
    "        removelist = set('for a of the and to in on is or be as where it its at an - with by'.split())\n",
    "        description = re.sub(r\"\\s+\", \" \", description) # remove line breaks and tabs\n",
    "        description = re.sub(r\"<.*?>\", \"\", description) # remove <>\n",
    "        description = description.replace(\",\",\"\") # remove commas\n",
    "        description = description.replace(\".\",\"\") # remove periods\n",
    "        # Split text into a list of words\n",
    "        text = [word for word in description.lower().split() if word not in removelist]\n",
    "        return text\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def compute_description_similarity(testRow,userGameDict):\n",
    "    # This function comutes the similarity of one game to the games a user already owns based on game description\n",
    "    tmp         = testRow.detailedescription.tolist()\n",
    "    query_doc   = preprocess_descriptions(tmp[0]) # preprocess data\n",
    "\n",
    "    # Compare to dictionary we computed on all other games\n",
    "    query_doc_bow = description_dictionary.doc2bow(query_doc)\n",
    "    # Vectorize word representation\n",
    "    query_doc_tf_idf = description_tfidf[query_doc_bow]\n",
    "    # Compute the similarity of the game with current tags to al games in our database\n",
    "    query_similarity = description_sims[query_doc_tf_idf]\n",
    "    \n",
    "    # Control for if the user has only one game\n",
    "    if type(userGameDict) == dict:\n",
    "        userGameDict = list([userGameDict])\n",
    "    \n",
    "    gamesOwned  = []\n",
    "    gamesPlayed = []\n",
    "    for g in userGameDict:\n",
    "        if g['appid'] in allGames:\n",
    "            #if g['appid'] != testGame: # If the user owns the testGame, we should omit this from the model\n",
    "            gamesOwned.append(g['appid'])\n",
    "            gamesPlayed.append(g['playtime_forever']+0.01)\n",
    "    # normalize the playtime to the total amount of playtime\n",
    "    gamesPlayed = np.array([x / sum(gamesPlayed) for x in gamesPlayed]) \n",
    "    \n",
    "    # We only want to compare to games that are in our original list\n",
    "    testInd = []\n",
    "    for game in gamesOwned:\n",
    "        if game in allGames:\n",
    "            testInd.append(allGames.index(game))\n",
    "    user2game_similarity = query_similarity[testInd]\n",
    "    user2game_weighted_similarity = user2game_similarity*gamesPlayed\n",
    "    \n",
    "    # If we just have empty arrays, then fill with 0\n",
    "    if user2game_similarity.size < 1:\n",
    "        user2game_similarity = 0\n",
    "        user2game_weighted_similarity = 0\n",
    "    \n",
    "    return np.median(user2game_similarity)#, np.median(user2game_weighted_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the similarity of hypothetical games to users games\n",
    "\n",
    "This code loops through each user and computes the tf-idf similarity of half-life 3 and euro trucker 3 tags and description to the games each user currently owns. The code calls a separate function that can be found above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's compute the similarity of hypothetical games to users currently owned games\n",
    "hl3_tag = []\n",
    "hl3_des = []\n",
    "et3_tag = []\n",
    "et3_des = []\n",
    "allGames = gameFrame.gameid.tolist()\n",
    "for index,user in testFrame.iterrows():\n",
    "    gd = ast.literal_eval(user.games)\n",
    "    hl3_tag.append(compute_tag_similarity(hl3frame,gd))\n",
    "    hl3_des.append(compute_description_similarity(hl3frame,gd))\n",
    "    et3_tag.append(compute_tag_similarity(euroTruck3,gd))\n",
    "    et3_des.append(compute_description_similarity(euroTruck3,gd))\n",
    "      \n",
    "    print(\"Processing user {}\".format(index),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring together user and hypothetical game information in the same dataframe\n",
    "\n",
    "In the cell below, I am making new dataframes with only the variables that I am interested in for both half-life 3 and eurotrucker 3, and inserting the similarity scores that I computed in the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make the half-life 3 dataframe first\n",
    "hframe    = pd.concat([hl3frame[['developers','final_price','init_price','metacriticscore','releasedate','recommendations','genre']]]*len(testFrame),axis=0)\n",
    "hframe    = hframe.reset_index()\n",
    "tf        = testFrame\n",
    "tf        = tf.reset_index()\n",
    "df_hl3    = pd.concat([tf,hframe],axis=1)\n",
    "df_hl3['description_similarity'] = hl3_des\n",
    "df_hl3['tag_similarity'] = hl3_tag  \n",
    "\n",
    "# Now let's make the dataframe for the eurotruck simulator 3\n",
    "eframe    = pd.concat([euroTruck3[['developers','final_price','init_price','metacriticscore','releasedate','recommendations','genre']]]*len(testFrame),axis=0)\n",
    "eframe    = eframe.reset_index()\n",
    "tf        = testFrame\n",
    "tf        = tf.reset_index()\n",
    "df_et3    = pd.concat([tf,eframe],axis=1)\n",
    "df_et3['description_similarity'] = et3_des\n",
    "df_et3['tag_similarity'] = et3_tag  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process dataframes \n",
    "\n",
    "The code below processes dataframes such that they are almost ready for the model. We need to turn categorical variables into dummy variables (new columns consisting of 0's and 1's) that the model can handle. In addition we have to perform some other preprocessing steps such that the XGBoost model can handle the data, this includes removing any strange characters (,[]<>) from columns names, and changing certain columns datatype to float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_columns(df):\n",
    "    # We also have to remove any weird characters from the dataframe column names\n",
    "    df.columns = [col.replace(',', '') for col in df.columns]\n",
    "    df.columns = [col.replace('[', '') for col in df.columns]\n",
    "    df.columns = [col.replace(']', '') for col in df.columns]\n",
    "    df.columns = [col.replace('<', '') for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def process_frame(df):\n",
    "    \n",
    "    # Generate dummy dataframes from our categorical variables    \n",
    "    dfCountry   = pd.get_dummies(df['usercountry'])\n",
    "    dfCountry   = preprocess_columns(dfCountry)\n",
    "    dfDeveloper = pd.get_dummies(df['developers'])\n",
    "    dfDeveloper = preprocess_columns(dfDeveloper)\n",
    "    dfGenre     = pd.get_dummies(df['genre'])\n",
    "    dfGenre     = preprocess_columns(dfGenre)\n",
    "\n",
    "    # Concatinate the dummy variables with the original dataframe\n",
    "    df_new      = pd.concat([df, dfCountry, dfDeveloper, dfGenre], axis=1)\n",
    "    # Remove the original categorical columns from the dataframe, keeping only the dummy variable columns\n",
    "    df_new = df_new.drop(['usercountry','developers','genre','tags'],axis=1)\n",
    "\n",
    "    # We have to convert the data type to float so that the XGBoost model can handle it\n",
    "    df_new['timecreated'] = pd.to_numeric(df_new['timecreated'],downcast='float')\n",
    "    df_new['friends'] = pd.to_numeric(df_new['friends'],downcast='float')\n",
    "    df_new['playTime'] = pd.to_numeric(df_new['playTime'],downcast='float')\n",
    "    df_new['numGames'] = pd.to_numeric(df_new['numGames'],downcast='float')\n",
    "    return df_new\n",
    "\n",
    "# Process the dataframes by creating dummy variabels etc.\n",
    "df_et3 = process_frame(df_et3)\n",
    "df_hl3 = process_frame(df_hl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataframes ready for passing to the model\n",
    "\n",
    "The model is expecting our data to be in a certain shape, with a particular number of columns with certain names, etc. The code below now takes our current dataframes for half-life 3 and euro trucker 3 and removed excess columns that were not in the training set, and adds those that are missing in the current dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_dummy_columns( d, columns ):\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0\n",
    "\n",
    "def fix_columns( d, columns ):  \n",
    "\n",
    "    add_missing_dummy_columns( d, columns )\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert( set( columns ) - set( d.columns ) == set())\n",
    "\n",
    "    extra_cols = set( d.columns ) - set( columns )\n",
    "    if extra_cols:\n",
    "        print(\"extra columns: {}\".format(extra_cols))\n",
    "\n",
    "    d = d[ columns ]\n",
    "    return d\n",
    "\n",
    "# Now we are going to load in some rows from the dataframe that was used to train the model.\n",
    "testSet  = pd.read_csv('/home/trainingDataset',nrows=20)\n",
    "del testSet['userHasGame']\n",
    "del testSet['Unnamed: 0']\n",
    "testCols = testSet.columns\n",
    "\n",
    "# Using the above functions, we will ensure that the test dataframes are in the shape the model is expecting\n",
    "df_et3 = fix_columns(df_et3,testCols)\n",
    "df_hl3 = fix_columns(df_hl3,testCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the model and predict how each user will respond to both games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in the model \n",
    "modelList = pickle.load(open('/home/ensemble_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hl3_pred  = np.zeros((len(modelList),len(df_hl3)))\n",
    "et3_pred  = np.zeros((len(modelList),len(df_et3)))\n",
    "hl3_prob  = np.zeros((len(modelList),len(df_hl3)))\n",
    "et3_prob  = np.zeros((len(modelList),len(df_et3)))\n",
    "for ind,model in enumerate(modelList):\n",
    "    tmp = model.predict_proba(df_hl3)\n",
    "    hl3_prob[ind,:] = tmp[:,1]\n",
    "    tmp = model.predict_proba(df_et3)\n",
    "    et3_prob[ind,:] = tmp[:,1]\n",
    "    et3_pred[ind,:] = model.predict(df_et3)\n",
    "    hl3_pred[ind,:] = model.predict(df_hl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hl3_predictions = np.round(np.median(hl3_pred,axis=0))\n",
    "et3_predictions = np.round(np.median(et3_pred,axis=0))\n",
    "hl3_probabilities = np.mean(hl3_prob,axis=0)\n",
    "et3_probabilities = np.mean(et3_prob,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the fraction of users that are predicted to purchase each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et3_percentage = round((sum(et3_predictions)/len(et3_predictions))*100,3)\n",
    "hl3_percentage = round((sum(hl3_predictions)/len(hl3_predictions))*100,3)\n",
    "print(\"{}% of users are predicted to buy Euro Trucker 3 Simulator\".format(et3_percentage))\n",
    "print(\"{}% of users are predicted to buy Half-Life 3\".format(hl3_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the probability distributions of how likely each user is to buy both games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = plt.figure(figsize=(11, 5))\n",
    "ax = plt.subplot()\n",
    "    \n",
    "# Make histogram of half life 3 sales probability distribution:\n",
    "weights = np.ones_like(hl3_probabilities)/float(len(hl3_probabilities))\n",
    "n, bins, patches = plt.hist(hl3_probabilities, bins=np.linspace(0,1,100), \n",
    "                            weights=weights, facecolor='blue', alpha=0.5,label='Half-Life 3')\n",
    "# Make histogram of Eurotruck 3 sales probability distribution:\n",
    "weights = np.ones_like(et3_probabilities)/float(len(et3_probabilities))\n",
    "n, bins, patches = plt.hist(et3_probabilities, bins=np.linspace(0,1,100),\n",
    "                            weights=weights,facecolor='red', alpha=0.5,label='Euro Truck Simulator 3')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(0.6, 1), loc=2, borderaxespad=0.)\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_xlabel('Probability users will buy game',fontsize=20)\n",
    "ax.set_ylabel('Fraction of users',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a funciton that will plot t-SNE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a functiont to perform a scatter plot on most played game\n",
    "def tSNE_scatter(x,ng,gameNames,gameID,mostPlayedGame,dotSize):\n",
    "    # Prepare the plot\n",
    "    f  = plt.figure(figsize=(8, 8),dpi=100)\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    palette = np.array(sns.color_palette(\"hls\", ng)) # produce some nice colors for the scatterplot\n",
    "    \n",
    "    groupedSC = []\n",
    "    grayColor = np.array([0.75,0.75,0.75]) # We will plot the 'other' datapoints in gray\n",
    "    # Plot all data points first\n",
    "    gameInd = [i for i, x in enumerate(mostPlayedGame) if x != gameNames]\n",
    "    sc = ax.scatter(x[gameInd,0], x[gameInd,1], lw=0, s=20, c=grayColor,label=\"Other\")\n",
    "    groupedSC.append(sc)\n",
    "    \n",
    "    # Now let's loop through and plot each of the most played games in the tSNE space\n",
    "    colInd = -1\n",
    "    for game in gameID:\n",
    "        colInd = colInd + 1\n",
    "        #print(colInd)\n",
    "        gameInd = [i for i, x in enumerate(mostPlayedGame) if x == game]\n",
    "        sc = ax.scatter(x[gameInd,0], x[gameInd,1], lw=0, s=dotSize, c=palette[colInd],label=gameNames[colInd])\n",
    "        groupedSC.append(sc)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=0, borderaxespad=0.,fontsize=20)\n",
    "    #ax.legend()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight') \n",
    "    return f, groupedSC, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot how much time users spend playing games\n",
    "\n",
    "Below I am just plotting the userPlaysGame matrix, which contains the amount of time (in hours) each user has played a list of games that has been released since 2010. You can see that the matrix is generally quite sparse, but one can observe both vertical and horizontal 'stripes' in the matrix indicating that users tend to play similar games a lot (vertical stripe), and certain usrs like to play a lot of games (horizontal stripes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 500\n",
    "testMat = userPlaysGame[:num,:]\n",
    "\n",
    "f  = plt.figure(figsize=(10, 5),dpi=100)\n",
    "sc = sns.heatmap(np.log10(testMat),vmin=0,vmax=3,cmap=\"Blues\",\n",
    "                 cbar_kws={'label': 'log$_{10}$(hours played)'}, yticklabels=False,xticklabels=False)\n",
    "sc.set_xlabel('Games',size=25)\n",
    "sc.set_ylabel('User',size=25)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute t-SNE on user playing time and game ownership matrices\n",
    "\n",
    "Here is a description of t-SNE that I lifted from the sklearn webpage:\n",
    "\n",
    "\"t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 20170912 # random initialization so we ge the same results each time we run the analysis\n",
    "positions = [] # This variable will be filled upon calling the TSNE function\n",
    "tSNE_playtime = TSNE(random_state=RS,perplexity=30).fit_transform(userPlaysGame)\n",
    "print(\"Finished userPlaysGame tSNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute a list for most played games on Steam\n",
    "\n",
    "We are going to compute a list of most played games across all of the users in our dataframe. This will be helpful for visualization of the Steam marketplace later when we plot the t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = Counter(mostPlayedGame)\n",
    "mpg   = count.most_common() # tuple of most played games\n",
    "cnt      = 0 # counter\n",
    "numGames = 20 # Number of games to plot in tSNE space\n",
    "gameNames = []\n",
    "gameID    = [] # Most played game\n",
    "for game in mpg:\n",
    "    if not gameFrame.loc[gameFrame['gameid'] == game[0]].empty: # Some app's are not games, and have empty fields in the pandas dataframe. So we skip those\n",
    "        cnt = cnt + 1\n",
    "        if cnt > numGames: # We only want a certain number of games\n",
    "            break\n",
    "        \n",
    "        # Store the game name and appid for each game in the top 20 most played list\n",
    "        tmp = gameFrame.loc[gameFrame['gameid'] == game[0]]['gamename']\n",
    "        gameNames.append(tmp.iloc[0])\n",
    "        gameID.append(game[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gl = np.array([1,3,4,5,7])\n",
    "top5_games = []\n",
    "top5_names = []\n",
    "for gi in gl:\n",
    "    top5_games.append(gameID[gi])\n",
    "    top5_names.append(gameNames[gi])\n",
    "testMPG = []\n",
    "for usr in mostPlayedGame:\n",
    "    if usr in top5_games:\n",
    "        testMPG.append(usr)\n",
    "    else:\n",
    "        testMPG.append(0)\n",
    "\n",
    "f,sc,ax = tSNE_scatter(tSNE_playtime,5,top5_names,top5_games,testMPG,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the t-SNE space and label each user by their most played game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,sc,ax = tSNE_scatter(tSNE_playtime,numGames,gameNames,gameID,mostPlayedGame,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot users second most played game as well\n",
    "\n",
    "Looking at the above plot it is apparent that most users favourite game is Counter Strike, however we still observe several clusters of (predominantly) Counter Strike users. To further break down this market representation of Steam user behavior, I am plotting below the second most played game for users who play Counter Strike the most. As you can see, this reveals more finer details about Steam user behavior. For example, the cluster in the lower right quadrant seems to be a group of users that play counter strike the most, but also play GTA V the second most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make another list of games, where for users who play CS:GO the most we plot their second favourite game\n",
    "s = []\n",
    "for i,u in enumerate(mostPlayedGame):\n",
    "    if u == 730:\n",
    "        s.append(secondMostPlayedGame[i])\n",
    "    else:\n",
    "        s.append(u)\n",
    "f,sc,ax = tSNE_scatter(tSNE_playtime,numGames,gameNames,gameID,s,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot counter strike play time in t-SNE space\n",
    "\n",
    "The above plot is informative, but it does not go all the way to explaining why we have such diversity in the t-SNE space when it comes to Counter Strike users. To get a better idea about these users behavior, I am plotting the t-SNE space, where the color of each dot indicates the total time each user has spent playing Counter Strike. Interetingly, it seems that the many of the cluster of CS:GO playes can be partially explained by the amount of time each player has spent playing that game (you can see by the consistent colours within each cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f  = plt.figure(figsize=(8, 8),dpi=100)\n",
    "ax = plt.subplot(aspect='equal') \n",
    "sc = ax.scatter(tSNE_playtime[:,0], tSNE_playtime[:,1], lw=0, s=15, c=csgoTime,cmap='plasma',vmin=-1,vmax=4)\n",
    "\n",
    "cbar = plt.colorbar(sc,orientation='horizontal',shrink=0.5)\n",
    "#cbar.ax.set_xticklabels(['Low', 'Medium', 'High'])  # horizontal colorbar\n",
    "cbar.set_label('log$_{10}$(hours playing CS:GO)')\n",
    "ax.axis('off')\n",
    "ax.axis('tight') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project sales predictions into t-SNE space\n",
    "\n",
    "To unserstand how Half-Life 3 will engage the Steam marketplace, we can plot the t-SNE space again, but this time colour coding each datapoint with the probability that each user will buy the hypothetical game. As you can see below, Half-Life 3 is predicted to engage a large proportion of the Steam marketplace, with the exception of a small cluster of users to the left of the plot (these are users that don't buy or play many games). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f  = plt.figure(figsize=(8, 8),dpi=100)\n",
    "ax = plt.subplot(aspect='equal') \n",
    "sc = ax.scatter(tSNE_playtime[:,0], tSNE_playtime[:,1], lw=0, s=20, c=hl3_probabilities,cmap='viridis',vmin=0.40,vmax=0.60)\n",
    "\n",
    "cbar = plt.colorbar(sc,orientation='horizontal',shrink=0.5,ticks=[0.4,0.5,0.6])\n",
    "#cbar.ax.set_xticklabels(['Low', 'Medium', 'High'])  # horizontal colorbar\n",
    "cbar.set_label('Probability user will buy Half-life 3')\n",
    "ax.axis('off')\n",
    "ax.axis('tight') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Euro Trucker Simulator 3 sales into t-SNE space\n",
    "\n",
    "As can be seen in the plot below, when we do the same for Euro Trucker 3 we see only very sparse engagement in the Steam market. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f  = plt.figure(figsize=(8, 8),dpi=100)\n",
    "ax = plt.subplot(aspect='equal') \n",
    "sc = ax.scatter(tSNE_playtime[:,0], tSNE_playtime[:,1], lw=0, s=20, c=et3_probabilities,cmap='viridis',vmin=0.3,vmax=0.5)\n",
    "\n",
    "cbar = plt.colorbar(sc,orientation='horizontal',shrink=0.5,ticks=[0.3,0.4,0.5])\n",
    "#cbar.ax.set_xticklabels(['Low', 'Medium', 'High'])  # horizontal colorbar\n",
    "cbar.set_label('Probability user will buy Euro Trucker Simulator 3')\n",
    "ax.axis('off')\n",
    "ax.axis('tight') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business case: explore parameters to maximize profit\n",
    "\n",
    "One of the tough decisions for game developers is to decide how to price each game. There needs to be a balance between the game price, the number of users who will be engaged, and of course profit. To illustrate how the tools I have developed can be used by game developers, I am going to vary the price of a theoretical Euro Trucker 3 game, while keeping everything else constant, and predict how many users will buy the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "prices = np.linspace(5,30,n)\n",
    "df_et3.head()\n",
    "\n",
    "score = np.zeros(n)\n",
    "\n",
    "testF = df_et3\n",
    "count = -1\n",
    "for price in prices:\n",
    "    count = count + 1\n",
    "    testF['final_price'] = price\n",
    "    testF['init_price'] = price\n",
    "    \n",
    "    tmp_pred  = np.zeros((len(modelList),len(df_hl3)))\n",
    "    for ind,model in enumerate(modelList):\n",
    "        tmp_pred[ind,:] = model.predict(testF)    \n",
    "    tmp_predictions = np.round(np.median(tmp_pred,axis=0))\n",
    "    score[count] = sum(tmp_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot game sales vs revenue\n",
    "\n",
    "It turns out that although more games will be sold if the game is priced at $10, the actual revenue for the developer studio is maximized when the game is sold at $15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSteamUsers = 125000000\n",
    "percent_users = score/len(testF)\n",
    "revenue = (percent_users*numSteamUsers)*prices\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,5),dpi=100)\n",
    "t = np.arange(0.01, 10.0, 0.01)\n",
    "ax1.plot(prices, percent_users, 'b-',label='Predicted game sales')\n",
    "ax1.set_xlabel('Euro Trucker Simulator 3 price ($)',fontsize=20)\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('% of all Steam users', color='b',fontsize=20)\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.grid()\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(prices, revenue/1000000, 'r',label='Projected revenue')\n",
    "ax2.set_ylabel('Dollars in millions', color='r',fontsize=20)\n",
    "ax2.tick_params('y', colors='r')\n",
    "ax2.grid()\n",
    "ax2.legend(loc=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkey patch the gradient descent script to store each iteration\n",
    "\n",
    "Here we are going to monkey patch the _gradient_descent function from the tSNE package in python. \n",
    "We will only change it very slightly, by adding one line to store the location of each data point\n",
    "across iterations - so we can track how the algorithm optimizes the separation of data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _gradient_descent(objective, p0, it, n_iter, n_iter_without_progress=50,\n",
    "                      momentum=0.5, learning_rate=200.0, min_gain=0.01,\n",
    "                      min_grad_norm=1e-7, min_error_diff=1e-7, verbose=0,\n",
    "                      n_iter_check=100,kwargs=None,objective_error=None, args=[]):\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    p = p0.copy().ravel()\n",
    "    update = np.zeros_like(p)\n",
    "    gains = np.ones_like(p)\n",
    "    error = np.finfo(np.float).max\n",
    "    best_error = np.finfo(np.float).max\n",
    "    best_iter = 0\n",
    "\n",
    "    for i in range(it, n_iter):\n",
    "        new_error, grad = objective(p, *args, **kwargs)\n",
    "        grad_norm = linalg.norm(grad)\n",
    "\n",
    "        inc = update * grad >= 0.0\n",
    "        dec = np.invert(inc)\n",
    "        gains[inc] += 0.05\n",
    "        gains[dec] *= 0.95\n",
    "        np.clip(gains, min_gain, np.inf)\n",
    "        grad *= gains\n",
    "        update = momentum * update - learning_rate * grad\n",
    "        p += update\n",
    "        \n",
    "        # This is the line we are adding to the \n",
    "        # function to store data point positions\n",
    "        #######################################\n",
    "        #######################################\n",
    "        positions.append(p.copy())  \n",
    "        #######################################\n",
    "        #######################################\n",
    "        \n",
    "        if (i + 1) % n_iter_check == 0:\n",
    "          \n",
    "            if new_error is None:\n",
    "                new_error = objective_error(p, *args)\n",
    "            error_diff = np.abs(new_error - error)\n",
    "            error = new_error\n",
    "\n",
    "            if verbose >= 2:\n",
    "                m = \"[t-SNE] Iteration %d: error = %.7f, gradient norm = %.7f\"\n",
    "                print(m % (i + 1, error, grad_norm))\n",
    "\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_iter = i\n",
    "            elif i - best_iter > n_iter_without_progress:\n",
    "                if verbose >= 2:\n",
    "                    print(\"[t-SNE] Iteration %d: did not make any progress \"\n",
    "                          \"during the last %d episodes. Finished.\"\n",
    "                          % (i + 1, n_iter_without_progress))\n",
    "                break\n",
    "            if grad_norm <= min_grad_norm:\n",
    "                if verbose >= 2:\n",
    "                    print(\"[t-SNE] Iteration %d: gradient norm %f. Finished.\"\n",
    "                          % (i + 1, grad_norm))\n",
    "                break\n",
    "            if error_diff <= min_error_diff:\n",
    "                if verbose >= 2:\n",
    "                    m = \"[t-SNE] Iteration %d: error difference %f. Finished.\"\n",
    "                    print(m % (i + 1, error_diff))\n",
    "                break\n",
    "\n",
    "        if new_error is not None:\n",
    "            error = new_error\n",
    "\n",
    "    return p, error, i\n",
    "sklearn.manifold.t_sne._gradient_descent = _gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recompute t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 20170912 # random initialization so we ge the same results each time we run the analysis\n",
    "positions = [] # This variable will be filled upon calling the TSNE function\n",
    "tSNE_playtime = TSNE(random_state=RS,perplexity=30).fit_transform(userPlaysGame)\n",
    "print(\"Finished userPlaysGame tSNE\")\n",
    "#tSNE_owns = TSNE(random_state=RS).fit_transform(userHasGame)\n",
    "#print(\"Finished userHasGame tSNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate animation of t-SNE computation and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iter = np.dstack(position.reshape(-1, 2) for position in positions)\n",
    "\n",
    "# Plot the first frame with the\n",
    "#ff, sc, ax = tSNE_scatter(X_iter[...,-1],numGames,gameNames,mostPlayedGame)\n",
    "ff, sc, ax = tSNE_scatter(X_iter[...,-1],5,top5_names,top5_games,testMPG,80)\n",
    "\n",
    "def make_frame_tsne(t):\n",
    "    \n",
    "    # Get the indices for data points for the 'other' condition\n",
    "    gameInd = [i for i, x in enumerate(mostPlayedGame) if x != gameNames]\n",
    "    i = int(t*40) # skip factor\n",
    "    x = X_iter[gameInd,:,i] # Pull out data\n",
    "    sc[0].set_offsets(x)\n",
    "\n",
    "    # Now let's loop through and plot each of the most played games in the tSNE space\n",
    "    colInd = 0\n",
    "    for game in top5_games:\n",
    "        colInd = colInd + 1\n",
    "        gameInd = [i for i, x in enumerate(mostPlayedGame) if x == game]\n",
    "        x = X_iter[gameInd,:,i]\n",
    "        sc[colInd].set_offsets(x)\n",
    "\n",
    "    return mplfig_to_npimage(ff)\n",
    "\n",
    "animation = mpy.VideoClip(make_frame_tsne,duration=X_iter.shape[2]/40.)\n",
    "animation.write_gif(\"/home/tSNE_animation_final.gif\", fps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
